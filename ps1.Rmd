---
title: 'Psych 254 W15 PS #1'
author: "Natalia Velez"
date: "January 11, 2015"
output: html_document
---

This is problem set #1, in which we hope you will practice the packages tidyr and dplyr. There are some great cheat sheets from [RStudio](http://www.rstudio.com/resources/cheatsheets/).

The data set
------------

This data set comes from a replication of [Janiszewski and Uy (2008)](http://dx.doi.org/10.1111/j.1467-9280.2008.02057.x), who investigated whether the precision of the anchor for a price influences the amount of adjustment.

In the data frame, the `Input.condition` variable represents the experimental condition (under the rounded anchor, the rounded anchor, over the rounded anchor). `Input.price1`, `Input.price2`, and `Input.price3` are the anchors for the `Answer.dog_cost`, `Answer.plasma_cost`, and `Answer.sushi_cost` items.

Preliminaries
-------------

I pretty much always clear the workspace and load the same basic helper functions before starting an analysis.

```{r prelims, warning=FALSE, message=FALSE}
# Change the following line to run on a different machine:
setwd("~/Documents/Stanford/Classes/W15/Psych 254/psych254_materials/analyses")
rm(list=ls())
source("../helper/useful.R")
```

Note that I'm using a "relative" path (the "../helper") rather than an absolute path (e.g. "/Users/mcfrank/code/projects/etc..."). The relative path means that someone else can run your code by changing to the right directory, while the absolute path will force someone else to make trivial changes every time they want to run it.

Part 1: Data cleaning
---------------------

The first part of this exercise actually just consists of getting the data in a format usable for analysis. This is not trivial. Let's try it:

```{r data1}
d <- read.csv("../data/janiszewski_rep_exercise.csv")

# I. Select which columns to keep
d <- select(d, WorkerId, Input.condition, Input.price1, Input.price2, 
            Input.price3, Answer.dog_cost, Answer.plasma_cost, 
            Answer.sushi_cost)

# II. Remove participants that completed multiple conditions
d <- d[!duplicated(d$WorkerId),]

# III. Tidy data
d.tidy <- d %>%
    rename(workerid = WorkerId,
         condition = Input.condition,
         plasma_anchor = Input.price1,
         dog_anchor = Input.price2,
         sushi_anchor = Input.price3,
         dog_cost = Answer.dog_cost,
         plasma_cost = Answer.plasma_cost, 
         sushi_cost = Answer.sushi_cost) %>%
  gather(name, cost, 
         dog_anchor, plasma_anchor, sushi_anchor, 
         dog_cost, plasma_cost, sushi_cost) %>%
  separate(name, c("item", "type"), sep = "_") %>%
  spread(type, cost)

# IV. Last touch: converted 'item' from char to factor (for consistency):
d.tidy$item <- as.factor(d.tidy$item)

summary(d.tidy)
str(d.tidy)
```

<div style="font-family: Times New Roman, serif">
**Solution:** The dataset had several issues:

1. Some participants had entered their estimates as a word (e.g., 'five hundred') rather than a number. Still others used commas in their estimates of larger numbers (e.g., $4,500), which prevented their responses from being converted to a numeric value.
2. Some participants responded more than once, completing the same task in different conditions.
3. There were several columns that were unnecessary for analysis (HIT ID, assignment ID, time submitted, time approved, location, etc.).

Because of these issues, the data frame was messy and difficult to analyze, and some columns that were supposed to be numeric (namely the 'cost' columns) were instead being read as factors.

The first issue was solved by manually fixing verbal responses and converting all cells to a consistent number format on Excel. The second issue was resolved using a combination of Excel and R. First, on Excel, I sorted all HITs by Worker ID and by submit time. Therefore, if any workers submitted more than one HIT, the very first HIT that they completed would appear first on the list. Then, using R, I deleted all rows containing a workerID that had appeared earlier in the data frame. This way, the first HIT submitted was kept for analysis, while the others were discarded. (This seems like a less conservative approach than the one used in the next part, in which all data from double-dipping participants is discarded.) Finally, the last issue was resolved by selecting which columns to keep using tidyr.
</div>

Part 2: Making these data tidy
------------------------------

Now let's start with the cleaned data, so that we are all beginning from the same place.

```{r data2}
d <- read.csv("../data/janiszewski_rep_cleaned.csv")
```

This data frame is in *wide* format - that means that each row is a participant and there are multiple observations per participant. This data is not *tidy*.

To make this data tidy, we'll do some cleanup. First, remove the columns you don't need, using the verb `select`.

HINT: `?select` and the examples of helper functions will help you be efficient.

**Solution:**
```{r select}
d.tidy <- select(d, WorkerId, starts_with("Input"), starts_with("Answer"))
```


Try renaming some variables using `rename`. A good naming scheme is:

* consistent with case
* consistent with "." or "_" ( "_" is usually preferred)
* concise as will be comprehensible to others

Try using the `%>%` operator as well. So you will be "piping" `d %>% rename(...)`.

**Solution:**
```{r rename}
d.tidy <- d.tidy %>%
  rename(workerid = WorkerId,
         condition = Input.condition,
         dog_anchor = Input.price1,
         plasma_anchor = Input.price2,
         sushi_anchor = Input.price3,
         dog_cost = Answer.dog_cost,
         plasma_cost = Answer.plasma_cost,
         sushi_cost = Answer.sushi_cost)
```


OK, now for the tricky part. Use the verb *gather* to turn this into a *tidy* data frame.

HINT: look for online examples!

**Solution:**
```{r gather}
d.tidy <- gather(d.tidy, name, cost,
                 dog_anchor, plasma_anchor, sushi_anchor,
                 dog_cost, plasma_cost, sushi_cost) %>%
          separate(name, c("item", "type"), sep = "_") %>%
          spread(type, cost)
head(d.tidy)
```


Bonus problem: *spread* these data back into a wide format data frame.

**Solution:**
```{r spread}
d.wide <- d.tidy %>%
  gather(type, value, anchor:cost) %>%
  mutate(item_type = paste(item, type, sep = "_")) %>%
  select(-item, -type) %>%
  spread(item_type, value)

head(d.wide)
```


Part 3: Manipulating the data using dplyr
-----------------------------------------

Try also using the dplyr `distinct` function to remove the duplicate participants from the raw csv file that you discovered in part 1.

```{r}
d.raw <- read.csv("../data/janiszewski_rep_exercise.csv")
d.unique.subs <- distinct(d.raw, WorkerId)
```

As we said in class, a good thing to do is always to check histograms of the response variable. Do that now, using either regular base graphics or ggplot. What can you conclude? 

```{r}
# COME BACK TO THIS!
# (1) Were participants evenly assigned to conditions?
assign.plot <- ggplot(d.unique.subs, aes(x = Input.condition, fill = Input.condition)) +
  geom_histogram() +
  xlab("Condition") +
  ylab("Frequency") +
  ggtitle("Condition assignment")

# (2) Are worker's responses all over the place?
dog.plot <- ggplot(d.unique.subs, aes(x = Answer.dog_cost, fill = Input.condition)) +
  geom_histogram() +
  xlab("Estimated cost") +
  ylab("Frequency") +
  guides(fill = F) +
  ggtitle("Estimated cost of a dog")

plasma.plot <- ggplot(d.unique.subs, aes(x = Answer.plasma_cost, fill = Input.condition)) +
  geom_histogram() +
  xlab("Estimated cost") +
  ylab("Frequency") +
  guides(fill = F) +
  ggtitle("Estimated cost of a plasma TV")

sushi.plot <- ggplot(d.unique.subs, aes(x = Answer.sushi_cost, fill = Input.condition)) +
  geom_histogram() +
  xlab("Estimated cost") +
  ylab("Frequency") +
  guides(fill = F) +
  ggtitle("Estimated cost of sushi")

multiplot(assign.plot, dog.plot, plasma.plot, sushi.plot, cols = 2)
```

<div style='font-family: Times New Roman, serif;'>
**Conclusion:** Participants were evenly assigned to conditions. Responses do not seem to follow a normal distribution. Therefore, the data may need to be scaled or compared using non-parametric tests. Using these plots, it is difficult to tell at a glance whether responses differ across conditions. At a glance, however, it seems that they are similar: for example, regardless of condition, people most commonly estimated that a dog costs about $2,000.
</div>

OK, now we turn to the actual data anlysis. We'll be using dplyr verbs to `filter`, `group`,`mutate`, and `summarise` the data.

Start by using `summarise` on `d.tidy` to compute the mean bet across all participants. Note that this is simply taking the grand mean. Ultimately, we would like to compute the mean for different conditions and items, but this will come later. Right now we're just learning the syntax of `summarise`.

```{r}
summarise(d.tidy, mean=mean(na.omit(cost)))
```

This is a great time to get comfortable with the `%>%` operator. In brief, `%>%` allows you to pipe data from one function to another. So if you would have written:

```{r eval=FALSE}
d <- function(d, other_stuff)
```
  
you can now write:

```{r eval=FALSE}
d <- d %>% function(other_stufF)
```

That doesn't seem like much, but it's cool when you can replace:

```{r eval=FALSE}
d <- function1(d, other_stuff)
d <- function2(d, lots_of_other_stuff, more_stuff)
d <- function3(d, yet_more_stuff)
```

with

```{r eval=FALSE}
d <- d %>% 
  function1(other_stuff) %>%
  function2(lots_of_other_stuff, more_stuff) %>%
  function3(yet_more_stuff)
```

In other words, you get to make a clean list of the things you want to do and chain them together without a lot of intermediate assignments. 

Let's use that capacity to combine `summarise` with `group_by`, which allows us to break up our summary into groups. Try grouping by item and condition and taking means using `summarise`, chaining these two verbs with `%>%`.

```{r}
group_by(na.omit(d.tidy), condition, item) %>%
  summarise(mean=mean(cost))
```

OK, it's looking like there are maybe some differences between conditions, but how are we going to plot these? They are fundamentally different magnitudes from one another. 

Really we need the size of the deviation from the anchor, which means we need the anchor value (the `Input.price` variables that we've ignored up until now). Let's go back to the data and add that in.

Take a look at this complex expression. You don't have to modify it, but see what is being done here with gather, separate and spread. Run each part (e.g. the first verb, the first two verbs, etc.) and after doing each, look at `head(d.tidy)` to see what they do. 

```{r}
d.tidy <- d %>%
  select(WorkerId, Input.condition, 
         starts_with("Answer"), 
         starts_with("Input")) %>%
  rename(workerid = WorkerId,
         condition = Input.condition,
         plasma_anchor = Input.price1,
         dog_anchor = Input.price2,
         sushi_anchor = Input.price3,
         dog_cost = Answer.dog_cost,
         plasma_cost = Answer.plasma_cost, 
         sushi_cost = Answer.sushi_cost) %>%
  gather(name, cost, 
         dog_anchor, plasma_anchor, sushi_anchor, 
         dog_cost, plasma_cost, sushi_cost) %>%
  separate(name, c("item", "type"), sep = "_") %>%
  spread(type, cost)
```

Now we can do the same thing as before but look at the relative difference between anchor and estimate. Let's do this two ways: 

* By computing absolute value of percentage change in price, and 
* By computing z-scores over items.

To do the first, use the `mutate` verb to add a percent change column, then compute the same summary as before. 

```{r}
pcts <- d.tidy %>%
  mutate(pct_change = abs((cost-anchor)/anchor)) %>%
  group_by(condition, item) %>%
  summarise(mean = mean(na.omit(pct_change)))

pcts
```

To do the second, you will need to `group` first by item, compute z-scores with respect to items, then further group by condition.

HINT: `scale(x)` returns a complicated data structure that doesn't play nicely with dplyr. try `scale(x)[,1]` to get what you need.

HINT: by default, `group_by` undoes any previous groupings. If you want to add new grouping variables *on top* of pre-existing ones, specify `add = TRUE`, e.g., `d %>% group_by(var1) %>% group_by(var2, add = TRUE)`.

```{r}
z.scores <- d.tidy %>% 
  group_by(item) %>%
  mutate(z = scale(cost)[,1]) %>%
  group_by(condition, add = T) %>%
  summarise(mean = mean(na.omit(z)))
```

OK, now here comes the end: we're going to plot the differences and see if anything happened. First the percent change:

```{r}
qplot(item, mean, fill=condition,
      position="dodge",
      stat="identity", geom="bar",
      data=pcts)
```

and the z-scores:

```{r}
qplot(item, mean, fill=condition,
      position="dodge",
      stat="identity", geom="bar",
      data=z.scores)
```

Oh well. This replication didn't seem to work out straightforwardly.